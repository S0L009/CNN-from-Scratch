{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():         \n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (28,28)) for x in tr_d[0][:15000]]\n",
    "    print(f\"train- {len(training_inputs)}\")\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1][:15000]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (28,28)) for x in va_d[0]]\n",
    "    print(f'valid- {len(validation_inputs)}')\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (28,28)) for x in te_d[0][:3000]]\n",
    "    print(f\"test- {len(test_inputs)}\")\n",
    "    test_results = [vectorized_result(y) for y in te_d[1][:3000]]\n",
    "    test_data = zip(test_inputs, test_results)\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train- 15000\n",
      "valid- 10000\n",
      "test- 3000\n"
     ]
    }
   ],
   "source": [
    "a=load_data_wrapper()\n",
    "a=list(a[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import correlate2d\n",
    "from scipy.signal import convolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "class Convolution:\n",
    "    def __init__(self, num_filters: int, size: tuple):\n",
    "        limit = np.sqrt(2.0 / (1 + num_filters))\n",
    "        self.weights = np.random.randn(num_filters, size[0], size[1]) * limit\n",
    "        self.weights=np.ones_like(self.weights)/9\n",
    "        self.biases = np.random.randn(num_filters, 26, 26) * limit\n",
    "        self.out_cache = None\n",
    "        self.Vw = False\n",
    "        self.Vb = False  # Momentum\n",
    "        self.Sw = False\n",
    "        self.Sb = False  # Rmsprop\n",
    "        self.chg_wt = np.zeros_like(self.weights)\n",
    "        self.chg_bs = np.zeros_like(self.biases)\n",
    "    def Fwd_Correlation(self, x):\n",
    "        std=pp.StandardScaler()\n",
    "        out = []\n",
    "        for i, j in zip(self.weights, self.biases):\n",
    "            out.append(std.fit_transform(correlate2d(x, i, mode='valid') + j))\n",
    "        self.out_cache = np.array(out)\n",
    "        return\n",
    "    def Bwd_Correlation(self, err, x):\n",
    "        for i in range(len(err)):\n",
    "            self.chg_wt[i] += convolve(x, err[i], mode='valid')\n",
    "            self.chg_bs[i] += err[i]\n",
    "        return\n",
    "    def Conv_update(self, lr, b_mom, b_rms, eps):\n",
    "        t=1\n",
    "        self.chg_bs/=50\n",
    "        self.chg_wt/=50\n",
    "        # self.Vw/=1-b_mom**t\n",
    "        # self.Vb/=1-b_mom**t\n",
    "        # self.Sw/=1-b_rms**t\n",
    "        # self.Sb/=1-b_rms**t\n",
    "        # self.Vw = b_mom * self.Vw + (1 - b_mom) * self.chg_wt\n",
    "        # self.Vb = b_mom * self.Vb + (1 - b_mom) * self.chg_bs\n",
    "        # self.Sw = b_rms * self.Sw + (1 - b_rms) * (self.chg_wt) ** 2\n",
    "        # self.Sb = b_rms * self.Sb + (1 - b_rms) * (self.chg_bs) ** 2\n",
    "        # self.weights -= lr * (self.Vw / np.sqrt(self.Sw + eps))\n",
    "        # self.biases -= lr * (self.Vb / np.sqrt(self.Sb + eps))\n",
    "        self.weights -= lr * self.chg_wt\n",
    "        self.biases -= lr *self.chg_bs\n",
    "        self.chg_wt = np.zeros_like(self.weights)\n",
    "        self.chg_bs = np.zeros_like(self.biases)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, prev_num: int, new_num: int):\n",
    "        limit = np.sqrt(6 / (prev_num + new_num))\n",
    "        self.weights = np.random.uniform(-limit, limit, size=(new_num, prev_num))\n",
    "        self.biases = np.random.randn(new_num, 1)\n",
    "        self.out_cache = None\n",
    "        self.chg_wt = None\n",
    "        self.chg_bs = None\n",
    "        self.Vw = False\n",
    "        self.Vb = False  # Momentum\n",
    "        self.Sw = False\n",
    "        self.Sb = False  # Rmsprop\n",
    "    def Fwd_Dense(self, x):\n",
    "        self.out_cache = np.dot(self.weights, x) + self.biases\n",
    "        return self.out_cache\n",
    "    def Bwd_Dense(self, err, dNdW):\n",
    "        dPdN = 1\n",
    "        dPdB = 1\n",
    "        if self.chg_bs is None:\n",
    "            self.chg_bs = err * dPdB\n",
    "            self.chg_wt = np.dot(err, dNdW.transpose()) * dPdN\n",
    "        self.chg_bs += err * dPdB\n",
    "        self.chg_wt += np.dot(err, dNdW.transpose()) * dPdB\n",
    "        return np.dot(self.weights.transpose(), err)\n",
    "    def Dense_update(self, lr, b_mom, b_rms, eps):\n",
    "        t=1\n",
    "        self.chg_bs/=50\n",
    "        self.chg_wt/=50\n",
    "        # self.Vw/=1-b_mom**t\n",
    "        # self.Vb/=1-b_mom**t\n",
    "        # self.Sw/=1-b_rms**t\n",
    "        # self.Sb/=1-b_rms**t\n",
    "        # self.Vw = b_mom * self.Vw + (1 - b_mom) * self.chg_wt\n",
    "        # self.Vb = b_mom * self.Vb + (1 - b_mom) * self.chg_bs\n",
    "        # self.Sw = b_rms * self.Sw + (1 - b_rms) * (self.chg_wt) ** 2\n",
    "        # self.Sb = b_rms * self.Sb + (1 - b_rms) * (self.chg_bs) ** 2\n",
    "        # self.weights -= lr * (self.Vw / np.sqrt(self.Sw + eps))\n",
    "        # self.biases -= lr * (self.Vb / np.sqrt(self.Sb + eps))\n",
    "        self.weights -= lr * self.chg_wt\n",
    "        self.biases -= lr * self.chg_bs\n",
    "        self.chg_bs = None\n",
    "        self.chg_wt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxpooling:\n",
    "    def __init__(self,size:tuple,stride:int)->None:\n",
    "        self.stride=stride\n",
    "        self.size=size[0]\n",
    "        self.coordinates=False\n",
    "        self.out_cache=False\n",
    "    def Fwd_Maxpool(self,ip):\n",
    "        val=[]\n",
    "        x=ip.shape\n",
    "        op=(x[0],x[1]//self.size,x[2]//self.stride)\n",
    "        crds=[]\n",
    "        dt={0:(0,0),1:(0,1),2:(1,0),3:(1,1)}\n",
    "        for s in range(0,ip.shape[0]):\n",
    "            for i in range(0,ip.shape[1]-1,self.stride):\n",
    "                for j in range(0,ip.shape[2]-1,self.stride):\n",
    "                    lt=[k for k in ip[s][i][j:j+self.stride]]\n",
    "                    for k in ip[s][i+1][j:j+self.stride]:\n",
    "                        lt.append(k)\n",
    "                    val.append(max(lt))\n",
    "                    b=dt[lt.index(val[-1])]\n",
    "                    crds.append((s,b[0]+i,j+b[1]))\n",
    "        val=np.array(val)\n",
    "        val=np.reshape(val,op)\n",
    "        self.coordinates=crds\n",
    "        self.out_cache=val\n",
    "        return\n",
    "    def Bwd_Maxpool(self,acvn_shp,flt2mat):\n",
    "        c=0\n",
    "        temp=np.zeros(acvn_shp)\n",
    "        for i in range(flt2mat.shape[0]):  \n",
    "            temp[(self.coordinates[c])]=flt2mat[i][0]\n",
    "            c+=1\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.out_cache=False \n",
    "    def Fwd_Relu(self,ip):\n",
    "        self.out_cache=np.maximum(0,ip)\n",
    "        return\n",
    "    def Bwd_Relu(self,ip):\n",
    "        return np.array(ip>0,dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.out_cache=False\n",
    "    def Fwd_Softmax(self,val):\n",
    "        mx=np.max(val)\n",
    "        val-=mx\n",
    "        self.out_cache=np.exp(val)/sum(np.exp(val))\n",
    "        return\n",
    "    def Bwd_Softmax(self,y):\n",
    "        label=np.argmax(y)\n",
    "        loss=self.out_cache-y                                   #-y/(self.out_cache + 10**-100)\n",
    "        s=np.sum(self.out_cache)\n",
    "        out=self.out_cache[label]*self.out_cache/(s**2)\n",
    "        out[label]=self.out_cache[label]*(s-self.out_cache[label])/(s**2)\n",
    "        return out*loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out_cache=False\n",
    "    def sigmoid_Fwd(self,val):\n",
    "        self.out_cache= 1.0/(1.0+np.exp(-val))\n",
    "        return self.out_cache\n",
    "    def sigmoid_Bwd(self,val):\n",
    "        return self.sigmoid_Fwd(val)*(1-self.sigmoid_Fwd(val))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have to imp batch norm and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,train,valid,test):\n",
    "        self.train=list(train)\n",
    "        self.test=list(test)\n",
    "        # self.valid=list(valid)\n",
    "        self.batch_size=50\n",
    "        self.epoch=9\n",
    "        self.learning_rate=0.01\n",
    "        self.beta_rms=0.999\n",
    "        self.beta_momentum=0.9\n",
    "        self.epsilon=10**-8\n",
    "        self.model_create=False\n",
    "        \n",
    "    def Start(self):\n",
    "        temp=len(self.train)/self.batch_size\n",
    "        for i in range(self.epoch):\n",
    "            np.random.shuffle(self.train)\n",
    "            lt=[self.train[i:i+self.batch_size] for i in np.arange(0,len(self.train),self.batch_size)]\n",
    "            count=0\n",
    "            global Loss\n",
    "            Loss=[]\n",
    "            for j in lt:\n",
    "                sys.stdout.flush()\n",
    "                sys.stdout.write(f\"\\r{self.batch_size} -> {count+1}/{temp}\")\n",
    "                self.MGD(j)\n",
    "                count+=1\n",
    "            print(f\"Loss -> {np.mean(np.array(Loss))}\\n\")\n",
    "            if self.test:\n",
    "                print(f\"Epoch {i}-> {self.evall(self.test)} / {len(self.test)}\")\n",
    "            else:\n",
    "                print(f\"Epoch {i}-> complete\")\n",
    "    \n",
    "    def MGD(self,x):\n",
    "        for i in x:\n",
    "            loss=self.model(x=i[0],y=i[1])\n",
    "            Loss.append(loss)\n",
    "        Conv1.Conv_update(self.learning_rate,self.beta_momentum,self.beta_rms,self.epsilon)\n",
    "        Dense1.Dense_update(self.learning_rate,self.beta_momentum,self.beta_rms,self.epsilon) \n",
    "        Dense2.Dense_update(self.learning_rate,self.beta_momentum,self.beta_rms,self.epsilon)\n",
    "        return  \n",
    "    \n",
    "    def model(self,x,y):\n",
    "        if not self.model_create:\n",
    "            global Conv1,Dense1,Dense2,Relu1,Relu2,Mxpl1,Softmax1\n",
    "            Conv1=Convolution(16,(3,3))\n",
    "            Relu1=Relu()\n",
    "            Mxpl1=Maxpooling((2,2),2)\n",
    "            Dense1=Dense(2704,100)   #5408 - 32*13x13, flatten removed to ignore extra calc's\n",
    "            Relu2=Relu()\n",
    "            # Relu2=Sigmoid()\n",
    "            Dense2=Dense(100,10)\n",
    "            Softmax1=Softmax()\n",
    "            self.model_create=True\n",
    "        \n",
    "        Conv1.Fwd_Correlation(x)\n",
    "        Relu1.Fwd_Relu(Conv1.out_cache)\n",
    "        Mxpl1.Fwd_Maxpool(Relu1.out_cache)\n",
    "        Flatten_op=np.reshape(Mxpl1.out_cache.flatten(),(-1,1))    #this one has to be included again\n",
    "        Dense1.Fwd_Dense(Flatten_op)\n",
    "        Relu2.Fwd_Relu(Dense1.out_cache)\n",
    "        Dense2.Fwd_Dense(Relu2.out_cache)\n",
    "        Softmax1.Fwd_Softmax(Dense2.out_cache)\n",
    "\n",
    "        loss=-sum(y*np.log(Softmax1.out_cache+ 10**-100))\n",
    "        combined_err_loss_softmax=Softmax1.Bwd_Softmax(y)   #y`n-yn\n",
    "        Dense_loss=Dense2.Bwd_Dense(combined_err_loss_softmax,Relu2.out_cache)\n",
    "        Relu_Loss=Relu2.Bwd_Relu(Dense_loss)\n",
    "        Dense_loss=Dense1.Bwd_Dense(Relu_Loss,Flatten_op)\n",
    "        Mxpl_Loss=Mxpl1.Bwd_Maxpool(Relu1.out_cache.shape,Dense_loss)\n",
    "        Relu_Loss=Relu1.Bwd_Relu(Mxpl_Loss)\n",
    "        Conv1.Bwd_Correlation(Relu_Loss,x)\n",
    "        return loss\n",
    "    \n",
    "    def evall(self,sett):\n",
    "        print('\\nChecking...')\n",
    "        lt=0\n",
    "        for i,j in sett:\n",
    "            Conv1.Fwd_Correlation(i)\n",
    "            Relu1.Fwd_Relu(Conv1.out_cache)\n",
    "            Mxpl1.Fwd_Maxpool(Relu1.out_cache)\n",
    "            Flatten_op=np.reshape(Mxpl1.out_cache.flatten(),(-1,1))    #this one has to be included again\n",
    "            Dense1.Fwd_Dense(Flatten_op)\n",
    "            Relu2.Fwd_Relu(Dense1.out_cache)\n",
    "            Dense2.Fwd_Dense(Relu2.out_cache)\n",
    "            Softmax1.Fwd_Softmax(Dense2.out_cache)\n",
    "            if np.argmax(Softmax1.out_cache)==np.argmax(j):lt+=1\n",
    "        return lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train- 15000\n",
      "valid- 10000\n",
      "test- 3000\n"
     ]
    }
   ],
   "source": [
    "dt=load_data_wrapper()\n",
    "ob=Network(dt[0],dt[1],dt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob.Start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
